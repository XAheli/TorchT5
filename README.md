# T5 From Scratch: A Complete PyTorch Implementation

Work in progress!!  (filled with errors rn)

<!-- A comprehensive, production-ready implementation of the T5 (Text-to-Text Transfer Transformer) model from scratch in PyTorch, following the original research paper specifications.

## ðŸŽ¯ Overview

This repository provides a complete implementation of T5 that includes:

- **Full T5 Architecture**: Encoder-decoder transformer with relative position encoding
- **Text-to-Text Framework**: Unified approach for all NLP tasks
- **Training Pipeline**: Pre-training and fine-tuning capabilities
- **Advanced Generation**: Beam search, top-k, top-p sampling
- **Production Ready**: Comprehensive testing, logging, and checkpointing

## ðŸ—ï¸ Architecture

Our implementation follows the original T5 paper specifications:

- **Encoder-Decoder Architecture**: Bidirectional encoder + autoregressive decoder
- **Relative Position Encoding**: T5's unique position bias mechanism
- **Pre-Norm Layer Normalization**: Applied before attention and feed-forward layers
- **Text-to-Text Format**: All tasks framed as text generation

### Model Sizes Supported

| Model | d_model | d_ff | num_layers | num_heads | Parameters |
|-------|---------|------|------------|-----------|------------|
| Small | 512     | 2048 | 6          | 8         | ~60M       |
| Base  | 768     | 3072 | 12         | 12        | ~220M      |
| Large | 1024    | 4096 | 24         | 16        | ~770M      |


## ðŸŽ¯ Features

### Model Features
- âœ… Full T5 architecture with relative position encoding
- âœ… Encoder-decoder attention mechanisms
- âœ… Text-to-text unified framework
- âœ… Multiple model sizes (Small, Base, Large)
- âœ… Gradient checkpointing for memory efficiency

### Training Features
- âœ… Pre-training with span corruption
- âœ… Fine-tuning for downstream tasks
- âœ… Mixed precision training support
- âœ… Learning rate scheduling with warmup
- âœ… Gradient clipping and accumulation
- âœ… Comprehensive logging and checkpointing

### Generation Features
- âœ… Greedy decoding
- âœ… Beam search
- âœ… Top-k and top-p sampling
- âœ… Temperature control
- âœ… Repetition penalty
- âœ… Length penalty

### Development Features
- âœ… Comprehensive test suite
- âœ… Type hints throughout
- âœ… Detailed documentation
- âœ… Example scripts and tutorials
- âœ… Configuration management

## ðŸ”¬ Research Compliance -->

This implementation strictly follows the original T5 paper:

> Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140), 1-67.

<!-- Key architectural choices:
- Pre-norm layer normalization placement
- Relative position bias computation
- Span corruption pre-training objective
- Text-to-text task formatting -->


