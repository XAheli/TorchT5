# T5 Model Configurations

t5_small:
  vocab_size: 32128
  d_model: 512
  d_kv: 64
  d_ff: 2048
  num_layers: 6
  num_heads: 8
  relative_attention_num_buckets: 32
  relative_attention_max_distance: 128
  dropout_rate: 0.1
  layer_norm_epsilon: 1e-6
  initializer_factor: 1.0
  feed_forward_proj: "relu"
  is_encoder_decoder: true
  use_cache: true
  pad_token_id: 0
  eos_token_id: 1
  decoder_start_token_id: 0

t5_base:
  vocab_size: 32128
  d_model: 768
  d_kv: 64
  d_ff: 3072
  num_layers: 12
  num_heads: 12
  relative_attention_num_buckets: 32
  relative_attention_max_distance: 128
  dropout_rate: 0.1
  layer_norm_epsilon: 1e-6
  initializer_factor: 1.0
  feed_forward_proj: "relu"
  is_encoder_decoder: true
  use_cache: true
  pad_token_id: 0
  eos_token_id: 1
  decoder_start_token_id: 0

training:
  batch_size: 8
  learning_rate: 1e-4
  num_epochs: 3
  warmup_steps: 1000
  weight_decay: 0.01
  gradient_clip_norm: 1.0
  save_steps: 1000
  eval_steps: 500
  logging_steps: 100
